{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filename: model\n",
    "\n",
    "#### Use the .randomSplit method to split the 311 data into training and test sets.\n",
    "#### Create a classification model to predict whether a case will be late or not (i.e. predict case_late).\n",
    "#### Experiment with different combinations of features and different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment and start spark session\n",
    "%matplotlib inline\n",
    "import pyspark\n",
    "import pyspark.ml\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cases, source, and department into spark dataframes\n",
    "df_cases = spark.read.csv('./sa311/case.csv', header=True, inferSchema=True)\n",
    "df_source = spark.read.csv('./sa311/source.csv', header=True, inferSchema=True)\n",
    "df_dept = spark.read.csv('./sa311/dept.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the databases\n",
    "df = df_cases.join(df_dept, on='dept_division', how='left')\n",
    "df = df.join(df_source, on='source_id', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- dept_division: string (nullable = true)\n",
      " |-- case_id: integer (nullable = true)\n",
      " |-- case_opened_date: string (nullable = true)\n",
      " |-- case_closed_date: string (nullable = true)\n",
      " |-- SLA_due_date: string (nullable = true)\n",
      " |-- case_late: string (nullable = true)\n",
      " |-- num_days_late: double (nullable = true)\n",
      " |-- case_closed: string (nullable = true)\n",
      " |-- service_request_type: string (nullable = true)\n",
      " |-- SLA_days: double (nullable = true)\n",
      " |-- case_status: string (nullable = true)\n",
      " |-- request_address: string (nullable = true)\n",
      " |-- council_district: integer (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- standardized_dept_name: string (nullable = true)\n",
      " |-- dept_subject_to_SLA: string (nullable = true)\n",
      " |-- source_username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ensure everything looks good thus far\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# clean the data a bit\n",
    "df = df.withColumn(\"num_days_late\", df[\"num_days_late\"].cast(DoubleType()))\\\n",
    "        .withColumn(\"SLA_days\", df[\"SLA_days\"].cast(DoubleType()))\\\n",
    "        .withColumn('dept_lower', F.lower(F.col('dept_name')))\\\n",
    "        .withColumn('dept_lower', regexp_replace(col('dept_lower'), r'[^a-z0-9]', '_'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          dept_lower| count|\n",
      "+--------------------+------+\n",
      "|trans___cap_impro...| 97841|\n",
      "|        city_council|    34|\n",
      "|animal_care_services|119362|\n",
      "|                null|   198|\n",
      "|development_services|  1397|\n",
      "|code_enforcement_...|321984|\n",
      "|solid_waste_manag...|286287|\n",
      "|parks_and_recreation| 19964|\n",
      "|        metro_health|  5313|\n",
      "|    customer_service|  2889|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at value counts for department to check for nulls\n",
    "df.groupBy('dept_lower').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nulls\n",
    "df = df.na.fill('none_associated', ['dept_lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          dept_lower| count|\n",
      "+--------------------+------+\n",
      "|     none_associated|   198|\n",
      "|trans___cap_impro...| 97841|\n",
      "|        city_council|    34|\n",
      "|animal_care_services|119362|\n",
      "|development_services|  1397|\n",
      "|code_enforcement_...|321984|\n",
      "|solid_waste_manag...|286287|\n",
      "|parks_and_recreation| 19964|\n",
      "|        metro_health|  5313|\n",
      "|    customer_service|  2889|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check again\n",
    "df.groupBy('dept_lower').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([.7, .3], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "rf = RFormula(formula = 'case_late ~ dept_lower')\n",
    "df_model = rf.fit(train).transform(train).select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|(9,[6],[1.0])|  0.0|\n",
      "|(9,[6],[1.0])|  0.0|\n",
      "|(9,[1],[1.0])|  0.0|\n",
      "|(9,[1],[1.0])|  0.0|\n",
      "|(9,[1],[1.0])|  0.0|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_fit = lr.fit(df_model)\n",
    "lr_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lr_fit.transform(df_model)\n",
    " .withColumn('we_got_it_right', col('label') == col('prediction'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = lr_fit.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary.areaUnderROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary.accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = rf.fit(test).transform(test).select('features', 'label')\n",
    "lr_fit_test = lr.fit(df_model)\n",
    "lr_fit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary = lr_fit.summary\n",
    "test_summary.areaUnderROC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
