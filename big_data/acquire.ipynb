{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## Using case.csv & dept.csv:\n",
    "\n",
    "#### 1. read into spark environment (df_case, df_dept)\n",
    "#### 2. write df_case and df_dept back to disk into their own directories (my_cases and my_depts)\n",
    "#### 3. Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)\n",
    "#### 4. Read your parquet files back into your spark environment.\n",
    "#### 5. Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)\n",
    "#### 6. Convert the pandas dataframes into spark dataframes (cases_sdf, depts_sdf)\n",
    "#### 7. Convert the spark dataframes back into pandas dataframes. (cases_pdf1, depts_pdf1)\n",
    "#### 8. Write the spark dataframes (cases_sdf, depts_sdf) to Hive tables.\n",
    "#### 9. Explore the Hive database/tables you have created using the methods in the lesson.\n",
    "#### 10. Read from the tables into two spark dataframes (cases_sdf, depts_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. read into spark environment (df_case, df_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[case_id: int, case_opened_date: string, case_closed_date: string, SLA_due_date: string, case_late: string, num_days_late: double, case_closed: string, dept_division: string, service_request_type: string, SLA_days: double, case_status: string, source_id: string, request_address: string, council_district: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(spark.read\n",
    " .option('header', True)\n",
    " .option('inferSchema', True)\n",
    " .format('csv')\n",
    " .load('./sa311/case.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case = (spark.read.csv('./sa311/case.csv', header=True, inferSchema=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept = spark.read.csv('./sa311/dept.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. write df_case and df_dept back to disk into their own directories (my_cases and my_depts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.write.csv('my_cases')\n",
    "df_dept.write.csv('my_depts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Write df_case and df_dept to parquet files (my_cases_parquet and my_depts_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_case.write.format('parquet').mode('overwrite').\\\n",
    "    option('header','true').save('sa311/my_caes_parquet')\n",
    "# typoed cases but whatever\n",
    "df_dept.write.format('parquet').mode('overwrite').\\\n",
    "    option('header','true').save('sa311/my_depts_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Read your parquet files back into your spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made the typo in 'caes' consistent...\n",
    "df_case = spark.read.format('parquet').\\\n",
    "    option(\"header\", True).\\\n",
    "    option(\"inferSchema\", True).\\\n",
    "    load(\"sa311/my_caes_parquet\")\n",
    "\n",
    "df_dept = spark.read.format('parquet').\\\n",
    "    option('header', True).\\\n",
    "    option('inferSchema', True).\\\n",
    "    load('sa311/my_depts_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Read case.csv and dept.csv into a pandas dataframe. (cases_pdf, depts_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_pdf = pd.read_csv('./sa311/case.csv')\n",
    "depts_pdf = pd.read_csv('./sa311/dept.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Convert the pandas dataframes into spark dataframes (cases_sdf, depts_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = spark.read.csv('./sa311/case.csv', header=True, inferSchema=True).schema\n",
    "cases_sdf = spark.createDataFrame(cases_pdf, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = spark.read.csv('./sa311/dept.csv', header=True, inferSchema=True).schema\n",
    "depts_sdf = spark.createDataFrame(depts_pdf, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Convert the spark dataframes back into pandas dataframes. (cases_pdf1, depts_pdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>case_opened_date</th>\n",
       "      <th>case_closed_date</th>\n",
       "      <th>SLA_due_date</th>\n",
       "      <th>case_late</th>\n",
       "      <th>num_days_late</th>\n",
       "      <th>case_closed</th>\n",
       "      <th>dept_division</th>\n",
       "      <th>service_request_type</th>\n",
       "      <th>SLA_days</th>\n",
       "      <th>case_status</th>\n",
       "      <th>source_id</th>\n",
       "      <th>request_address</th>\n",
       "      <th>council_district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1014127332</td>\n",
       "      <td>1/1/18 0:42</td>\n",
       "      <td>1/1/18 12:29</td>\n",
       "      <td>9/26/20 0:42</td>\n",
       "      <td>NO</td>\n",
       "      <td>-998.508762</td>\n",
       "      <td>YES</td>\n",
       "      <td>Field Operations</td>\n",
       "      <td>Stray Animal</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>Closed</td>\n",
       "      <td>svcCRMLS</td>\n",
       "      <td>2315  EL PASO ST, San Antonio, 78207</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1014127333</td>\n",
       "      <td>1/1/18 0:46</td>\n",
       "      <td>1/3/18 8:11</td>\n",
       "      <td>1/5/18 8:30</td>\n",
       "      <td>NO</td>\n",
       "      <td>-2.012604</td>\n",
       "      <td>YES</td>\n",
       "      <td>Storm Water</td>\n",
       "      <td>Removal Of Obstruction</td>\n",
       "      <td>4.322222</td>\n",
       "      <td>Closed</td>\n",
       "      <td>svcCRMSS</td>\n",
       "      <td>2215  GOLIAD RD, San Antonio, 78223</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1014127334</td>\n",
       "      <td>1/1/18 0:48</td>\n",
       "      <td>1/2/18 7:57</td>\n",
       "      <td>1/5/18 8:30</td>\n",
       "      <td>NO</td>\n",
       "      <td>-3.022338</td>\n",
       "      <td>YES</td>\n",
       "      <td>Storm Water</td>\n",
       "      <td>Removal Of Obstruction</td>\n",
       "      <td>4.320729</td>\n",
       "      <td>Closed</td>\n",
       "      <td>svcCRMSS</td>\n",
       "      <td>102  PALFREY ST W, San Antonio, 78223</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1014127335</td>\n",
       "      <td>1/1/18 1:29</td>\n",
       "      <td>1/2/18 8:13</td>\n",
       "      <td>1/17/18 8:30</td>\n",
       "      <td>NO</td>\n",
       "      <td>-15.011481</td>\n",
       "      <td>YES</td>\n",
       "      <td>Code Enforcement</td>\n",
       "      <td>Front Or Side Yard Parking</td>\n",
       "      <td>16.291887</td>\n",
       "      <td>Closed</td>\n",
       "      <td>svcCRMSS</td>\n",
       "      <td>114  LA GARDE ST, San Antonio, 78223</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1014127336</td>\n",
       "      <td>1/1/18 1:34</td>\n",
       "      <td>1/1/18 13:29</td>\n",
       "      <td>1/1/18 4:34</td>\n",
       "      <td>YES</td>\n",
       "      <td>0.372164</td>\n",
       "      <td>YES</td>\n",
       "      <td>Field Operations</td>\n",
       "      <td>Animal Cruelty(Critical)</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>Closed</td>\n",
       "      <td>svcCRMSS</td>\n",
       "      <td>734  CLEARVIEW DR, San Antonio, 78228</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_id case_opened_date case_closed_date  SLA_due_date case_late  \\\n",
       "0  1014127332      1/1/18 0:42     1/1/18 12:29  9/26/20 0:42        NO   \n",
       "1  1014127333      1/1/18 0:46      1/3/18 8:11   1/5/18 8:30        NO   \n",
       "2  1014127334      1/1/18 0:48      1/2/18 7:57   1/5/18 8:30        NO   \n",
       "3  1014127335      1/1/18 1:29      1/2/18 8:13  1/17/18 8:30        NO   \n",
       "4  1014127336      1/1/18 1:34     1/1/18 13:29   1/1/18 4:34       YES   \n",
       "\n",
       "   num_days_late case_closed     dept_division        service_request_type  \\\n",
       "0    -998.508762         YES  Field Operations                Stray Animal   \n",
       "1      -2.012604         YES       Storm Water      Removal Of Obstruction   \n",
       "2      -3.022338         YES       Storm Water      Removal Of Obstruction   \n",
       "3     -15.011481         YES  Code Enforcement  Front Or Side Yard Parking   \n",
       "4       0.372164         YES  Field Operations    Animal Cruelty(Critical)   \n",
       "\n",
       "     SLA_days case_status source_id                        request_address  \\\n",
       "0  999.000000      Closed  svcCRMLS   2315  EL PASO ST, San Antonio, 78207   \n",
       "1    4.322222      Closed  svcCRMSS    2215  GOLIAD RD, San Antonio, 78223   \n",
       "2    4.320729      Closed  svcCRMSS  102  PALFREY ST W, San Antonio, 78223   \n",
       "3   16.291887      Closed  svcCRMSS   114  LA GARDE ST, San Antonio, 78223   \n",
       "4    0.125000      Closed  svcCRMSS  734  CLEARVIEW DR, San Antonio, 78228   \n",
       "\n",
       "   council_district  \n",
       "0                 5  \n",
       "1                 3  \n",
       "2                 3  \n",
       "3                 3  \n",
       "4                 7  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases_pdf1 = cases_sdf.toPandas()\n",
    "cases_pdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_pdf1 = depts_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dept_division</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>standardized_dept_name</th>\n",
       "      <th>dept_subject_to_SLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311 Call Center</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brush</td>\n",
       "      <td>Solid Waste Management</td>\n",
       "      <td>Solid Waste</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clean and Green</td>\n",
       "      <td>Parks and Recreation</td>\n",
       "      <td>Parks &amp; Recreation</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clean and Green Natural Areas</td>\n",
       "      <td>Parks and Recreation</td>\n",
       "      <td>Parks &amp; Recreation</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Code Enforcement</td>\n",
       "      <td>Code Enforcement Services</td>\n",
       "      <td>DSD/Code Enforcement</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dept_division                  dept_name  \\\n",
       "0                311 Call Center           Customer Service   \n",
       "1                          Brush     Solid Waste Management   \n",
       "2                Clean and Green       Parks and Recreation   \n",
       "3  Clean and Green Natural Areas       Parks and Recreation   \n",
       "4               Code Enforcement  Code Enforcement Services   \n",
       "\n",
       "  standardized_dept_name dept_subject_to_SLA  \n",
       "0       Customer Service                 YES  \n",
       "1            Solid Waste                 YES  \n",
       "2     Parks & Recreation                 YES  \n",
       "3     Parks & Recreation                 YES  \n",
       "4   DSD/Code Enforcement                 YES  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depts_pdf1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Write the spark dataframes (cases_sdf, depts_sdf) to Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "table_name1 = 'df_' + str(uuid.uuid4().hex)\n",
    "table_name2 = 'df_' + str(uuid.uuid4().hex)\n",
    "cases_sdf.write.saveAsTable(table_name1)\n",
    "depts_sdf.write.saveAsTable(table_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Explore the Hive database/tables you have created using the methods in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|             case_id|      int|   null|\n",
      "|    case_opened_date|   string|   null|\n",
      "|    case_closed_date|   string|   null|\n",
      "|        SLA_due_date|   string|   null|\n",
      "|           case_late|   string|   null|\n",
      "|       num_days_late|   double|   null|\n",
      "|         case_closed|   string|   null|\n",
      "|       dept_division|   string|   null|\n",
      "|service_request_type|   string|   null|\n",
      "|            SLA_days|   double|   null|\n",
      "|         case_status|   string|   null|\n",
      "|           source_id|   string|   null|\n",
      "|     request_address|   string|   null|\n",
      "|    council_district|      int|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "df_c851995fdc3347fabb097dd8a33ed63d\n"
     ]
    }
   ],
   "source": [
    "query1 = \"DESCRIBE %s\" % table_name1\n",
    "query2 = \"DESCRIBE %s\" % table_name2\n",
    "spark.sql(query1).show()\n",
    "print(table_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|       dept_division|   string|   null|\n",
      "|           dept_name|   string|   null|\n",
      "|standardized_dept...|   string|   null|\n",
      "| dept_subject_to_SLA|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "df_e7e74eda954544419c8fa5b6e6e61689\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query2).show()\n",
    "print(table_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Read from the tables into two spark dataframes (cases_sdf, depts_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "| default|df_c851995fdc3347...|      false|\n",
      "| default|df_e7e74eda954544...|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE default\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|             case_id|      int|   null|\n",
      "|    case_opened_date|   string|   null|\n",
      "|    case_closed_date|   string|   null|\n",
      "|        SLA_due_date|   string|   null|\n",
      "|           case_late|   string|   null|\n",
      "|       num_days_late|   double|   null|\n",
      "|         case_closed|   string|   null|\n",
      "|       dept_division|   string|   null|\n",
      "|service_request_type|   string|   null|\n",
      "|            SLA_days|   double|   null|\n",
      "|         case_status|   string|   null|\n",
      "|           source_id|   string|   null|\n",
      "|     request_address|   string|   null|\n",
      "|    council_district|      int|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'DESCRIBE {table_name1}').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|       dept_division|   string|   null|\n",
      "|           dept_name|   string|   null|\n",
      "|standardized_dept...|   string|   null|\n",
      "| dept_subject_to_SLA|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'DESCRIBE {table_name2}').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_sdf = spark.sql(f'SELECT * FROM {table_name1}')\n",
    "depts_sdf = spark.sql(f'SELECT * FROM {table_name2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1 = 'DROP TABLE IF EXISTS %s' % table_name1\n",
    "query2 = 'DROP TABLE IF EXISTS %s' % table_name2\n",
    "\n",
    "spark.sql(query1)\n",
    "spark.sql(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
